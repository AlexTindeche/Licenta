{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counts people depending on the colour of the traffic ligth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervision.__version__: 0.1.0\n",
      "YOLOv8x summary (fused): 268 layers, 68200608 parameters, 0 gradients, 257.8 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "###################### Imports #######################\n",
    "\n",
    "\n",
    "###############\n",
    "# Ultralytics #\n",
    "###############\n",
    "\n",
    "# !pip install ultralytics\n",
    "\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "\n",
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "\n",
    "######################\n",
    "# Set Home Directory #\n",
    "######################\n",
    "\n",
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)\n",
    "\n",
    "######################\n",
    "# Import yolox modek #\n",
    "######################\n",
    "\n",
    "# %cd {HOME}\n",
    "# !git clone https://github.com/ifzhang/ByteTrack.git\n",
    "# %cd {HOME}/ByteTrack\n",
    "\n",
    "# # workaround related to https://github.com/roboflow/notebooks/issues/80\n",
    "# !sed -i 's/onnx==1.8.1/onnx==1.9.0/g' requirements.txt\n",
    "\n",
    "# !pip3 install -q -r requirements.txt\n",
    "# !python3 setup.py -q develop\n",
    "# !pip install -q cython_bbox\n",
    "# !pip install -q onemetric\n",
    "# # workaround related to https://github.com/roboflow/notebooks/issues/112 and https://github.com/roboflow/notebooks/issues/106\n",
    "# !pip install -q loguru lap thop\n",
    "\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(f\"{HOME}/ByteTrack\")\n",
    "\n",
    "\n",
    "import yolox\n",
    "print(\"yolox.__version__:\", yolox.__version__)\n",
    "\n",
    "##########################\n",
    "# Import ByteTrack model #\n",
    "##########################\n",
    "\n",
    "from yolox.tracker.byte_tracker import BYTETracker, STrack\n",
    "from onemetric.cv.utils.iou import box_iou_batch\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class BYTETrackerArgs:\n",
    "    track_thresh: float = 0.10\n",
    "    track_buffer: int = 50\n",
    "    match_thresh: float = 0.8\n",
    "    aspect_ratio_thresh: float = 3.0\n",
    "    min_box_area: float = 1.0\n",
    "    mot20: bool = False\n",
    "\n",
    "\n",
    "######################\n",
    "# Import Supervision #\n",
    "######################\n",
    "    \n",
    "# !pip install supervision==0.1.0\n",
    "\n",
    "\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "\n",
    "\n",
    "import supervision\n",
    "print(\"supervision.__version__:\", supervision.__version__)\n",
    "\n",
    "from supervision.draw.color import ColorPalette\n",
    "from supervision.draw.color import Color\n",
    "from supervision.geometry.dataclasses import Point\n",
    "from supervision.video.dataclasses import VideoInfo\n",
    "from supervision.video.source import get_video_frames_generator\n",
    "from supervision.video.sink import VideoSink\n",
    "from supervision.notebook.utils import show_frame_in_notebook\n",
    "from supervision.tools.detections import Detections, BoxAnnotator\n",
    "from supervision.tools.line_counter import LineCounter, LineCounterAnnotator\n",
    "\n",
    "####################### Setups #######################\n",
    "\n",
    "######################\n",
    "# ByteTrack settings #\n",
    "######################\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# converts Detections into format that can be consumed by match_detections_with_tracks function\n",
    "def detections2boxes(detections: Detections) -> np.ndarray:\n",
    "    return np.hstack((\n",
    "        detections.xyxy,\n",
    "        detections.confidence[:, np.newaxis]\n",
    "    ))\n",
    "\n",
    "\n",
    "# converts List[STrack] into format that can be consumed by match_detections_with_tracks function\n",
    "def tracks2boxes(tracks: List[STrack]) -> np.ndarray:\n",
    "    return np.array([\n",
    "        track.tlbr\n",
    "        for track\n",
    "        in tracks\n",
    "    ], dtype=float)\n",
    "\n",
    "\n",
    "# matches our bounding boxes with predictions\n",
    "def match_detections_with_tracks(\n",
    "    detections: Detections,\n",
    "    tracks: List[STrack]\n",
    ") -> Detections:\n",
    "    if not np.any(detections.xyxy) or len(tracks) == 0:\n",
    "        return np.empty((0,))\n",
    "\n",
    "    tracks_boxes = tracks2boxes(tracks=tracks)\n",
    "    iou = box_iou_batch(tracks_boxes, detections.xyxy)\n",
    "    track2detection = np.argmax(iou, axis=1)\n",
    "\n",
    "    tracker_ids = [None] * len(detections)\n",
    "\n",
    "    for tracker_index, detection_index in enumerate(track2detection):\n",
    "        if iou[tracker_index, detection_index] != 0:\n",
    "            tracker_ids[detection_index] = tracks[tracker_index].track_id\n",
    "\n",
    "    return tracker_ids\n",
    "\n",
    "\n",
    "##################\n",
    "# YoloX settings #\n",
    "##################\n",
    "from ultralytics import YOLO\n",
    "\n",
    "MODEL = \"yolov8x.pt\"\n",
    "\n",
    "\n",
    "model = YOLO(MODEL)\n",
    "model.fuse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths and other constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE_VIDEO_PATH: ../Videos/00008-US.mp4\n",
      "TARGET_VIDEO_PATH: /mnt/Storage/Licenta-main/Licenta/res-01-US.mp4\n"
     ]
    }
   ],
   "source": [
    "###################### Paths #######################\n",
    "\n",
    "SOURCE_VIDEO_PATH = \"../Videos/00008-US.mp4\"\n",
    "\n",
    "TARGET_VIDEO_PATH = f\"{HOME}/res-01-US.mp4\"\n",
    "\n",
    "print(f\"SOURCE_VIDEO_PATH: {SOURCE_VIDEO_PATH}\")\n",
    "print(f\"TARGET_VIDEO_PATH: {TARGET_VIDEO_PATH}\")\n",
    "\n",
    "VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
    "\n",
    "###################### Class constants #######################\n",
    "\n",
    "# dict maping class_id to class_name\n",
    "CLASS_NAMES_DICT = model.model.names\n",
    "# class_ids of interest - person, car, motorcycle, bus and truck\n",
    "CLASS_ID = [0, 2, 3, 5, 7]\n",
    "CLASS_ID_PEOPLE = [0]\n",
    "CLASS_ID_VEHICLE = [2, 3, 5, 7]\n",
    "CLASS_ID_TRAFFIC_LIGHT = [9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select areas of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polygon 1: ['(156, 386)', '(892, 335)', '(654, 216)', '(134, 202)']\n",
      "Polygon 2: ['(0, 373)', '(303, 357)', '(462, 673)', '(29, 683)']\n",
      "Polygon 3: ['(160, 609)', '(1253, 495)', '(1251, 709)', '(211, 710)']\n",
      "Polygon 4: ['(729, 341)', '(1232, 306)', '(1274, 532)', '(1098, 528)']\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class Point:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "def select_polygons(image):\n",
    "    \"\"\"\n",
    "    Allows the user to select polygons on an image using mouse clicks until 'Enter' is pressed.\n",
    "    \n",
    "    :param image: The image on which polygons will be selected.\n",
    "    \"\"\"\n",
    "    # Initialize global variables\n",
    "    points = []  # To store the points for the current polygon\n",
    "    polygons = []  # To store all polygons\n",
    "\n",
    "    resized_img = cv2.resize(frame, (1000, 800))\n",
    "\n",
    "    # Calculate scale factors\n",
    "    originalHeight, originalWidth = frame.shape[:2]\n",
    "    scaleFactorX = originalWidth / 1000\n",
    "    scaleFactorY = originalHeight / 800\n",
    "\n",
    "\n",
    "    def click_event(event, x, y, flags, param):\n",
    "        nonlocal points, image\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:  # Left button click\n",
    "            # scale the points back to the original image size\n",
    "            x_scaled = int(x * scaleFactorX)\n",
    "            y_scaled = int(y * scaleFactorY)\n",
    "            points.append(Point(x_scaled, y_scaled))\n",
    "            cv2.circle(resized_img, (x, y), 5, (0, 0, 255), -1)  # Draw the dot\n",
    "            if len(points) > 1:\n",
    "                cv2.line(resized_img, \n",
    "                    (int(points[-2].x // scaleFactorX), int(points[-2].y // scaleFactorY)),\n",
    "                    (int(points[-1].x // scaleFactorX), int(points[-1].y // scaleFactorY)), \n",
    "                    (255, 0, 0), 2)\n",
    "            cv2.imshow(\"image\", resized_img)  # Show the image with the new dot/line\n",
    "            \n",
    "        elif event == cv2.EVENT_RBUTTONDOWN:  # Right button to finish the polygon\n",
    "            if len(points) > 2:  # Need at least 3 points to form a polygon\n",
    "                cv2.line(resized_img, \n",
    "                    (int(points[-1].x // scaleFactorX), int(points[-1].y // scaleFactorY)),\n",
    "                    (int(points[0].x // scaleFactorX), int(points[0].y // scaleFactorY)), \n",
    "                    (255, 0, 0), 2)\n",
    "                polygons.append(points.copy())  # Store the completed polygon\n",
    "                points.clear()  # Reset points for the next polygon\n",
    "            cv2.imshow(\"image\", resized_img)\n",
    "\n",
    "    cv2.namedWindow(\"image\")\n",
    "    cv2.setMouseCallback(\"image\", click_event)\n",
    "    cv2.imshow(\"image\", resized_img)\n",
    "\n",
    "    while True:  # Loop to keep the window open until 'Enter' is pressed\n",
    "        key = cv2.waitKey(0) & 0xFF\n",
    "        if key == 13:  # ASCII code for Enter key\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    return polygons\n",
    "\n",
    "generator = get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "iterator = iter(generator)\n",
    "frame = next(iterator)\n",
    "polygons = select_polygons(frame)\n",
    "\n",
    "for i, polygon in enumerate(polygons):\n",
    "    print(f\"Polygon {i+1}: {[f'({p.x}, {p.y})' for p in polygon]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to count vehicles that go from a zone to another\n",
    "\n",
    "from utils.polygon import Polygon\n",
    "\n",
    "def count_trajectories(traffic_zones, detections, previous_placements):\n",
    "    # Define traffic zones trajectories\n",
    "    # 1-2, 1-3, 1-4, 2-3, 2-4, 3-4, etc\n",
    "    # Transform traffic zones into polygons\n",
    "    polygons = [Polygon(polygon) for polygon in traffic_zones]\n",
    "    nr_zones = len(polygons)\n",
    "    counts = dict()\n",
    "    for i in range(nr_zones):\n",
    "        counts[i] = 0\n",
    "\n",
    "    # See in which zone the detection is\n",
    "    # For every detection verify all the polygons and see if it is in one of them\n",
    "    detection_in_zone = dict()\n",
    "    for xyxy, confidence, class_id, tracker_id in detections:\n",
    "        # If there is no tracker_id, we skip the detection\n",
    "        if tracker_id is None:\n",
    "            continue\n",
    "\n",
    "        x1, y1, x2, y2 = xyxy\n",
    "        # anchors = [\n",
    "        #     Point(x=x1, y=y1),\n",
    "        #     Point(x=x1, y=y2),\n",
    "        #     Point(x=x2, y=y1),\n",
    "        #     Point(x=x2, y=y2),\n",
    "        # ]\n",
    "\n",
    "        # Workaround for the fact that the bboxes are too big\n",
    "\n",
    "        '''\n",
    "            Introduce a padding so the bbox will be the one in the interior of the original one, like so:\n",
    "            |-----------------------|\n",
    "            |  \\                  / |\n",
    "            |   \\---------------/   |\n",
    "            |   |Bbox w padding |   |\n",
    "            |   /---------------\\   |\n",
    "            | /                  \\  |\n",
    "            |-----------------------|\n",
    "        \n",
    "        '''\n",
    "        # Preduce the box by 40% of its original size\n",
    "        percentange = 0.3\n",
    "        anchors = [\n",
    "            Point(x=x1 + (x2 - x1) * percentange, y=y1 + (y2 - y1) * percentange),\n",
    "            Point(x=x1 + (x2 - x1) * percentange, y=y2 - (y2 - y1) * percentange),\n",
    "            Point(x=x2 - (x2 - x1) * percentange, y=y1 + (y2 - y1) * percentange),\n",
    "            Point(x=x2 - (x2 - x1) * percentange, y=y2 - (y2 - y1) * percentange),\n",
    "        ]\n",
    "\n",
    "        for i, polygon in enumerate(polygons):\n",
    "            ok = True\n",
    "            # For every side of the polygon chech if the 4 points are inside the polygon\n",
    "            for anchor in anchors:\n",
    "                if not polygon.is_in(anchor):\n",
    "                    ok = False\n",
    "                    break\n",
    "            if ok:\n",
    "                # If the detection is in the polygon, increment the counter\n",
    "                detection_in_zone[tracker_id] = i\n",
    "\n",
    "\n",
    "        # Check if the detection changed the zone\n",
    "        # PS: We ignore if the detection is in the same zone or the detection system didn't assign it to a zone\n",
    "        if tracker_id in previous_placements:\n",
    "            if tracker_id in detection_in_zone and detection_in_zone[tracker_id] != previous_placements.get(tracker_id, -1) \\\n",
    "            and (detection_in_zone[tracker_id] is not None or i >= 0):\n",
    "                if (previous_placements[tracker_id], detection_in_zone[tracker_id]) not in counts:\n",
    "                    counts[(previous_placements[tracker_id], detection_in_zone[tracker_id])] = 1\n",
    "                elif previous_placements.get(tracker_id, -1) >= 0:\n",
    "                    counts[(previous_placements[tracker_id], detection_in_zone[tracker_id])] += 1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    return counts, detection_in_zone, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on whole video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d2a1fc868e4f3687db44bf2c3f04df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 59\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m VideoSink(TARGET_VIDEO_PATH, video_info) \u001b[38;5;28;01mas\u001b[39;00m sink:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# loop over video frames\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m frame_nr, frame \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(generator, total\u001b[38;5;241m=\u001b[39mvideo_info\u001b[38;5;241m.\u001b[39mtotal_frames), start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):        \u001b[38;5;66;03m# model prediction on single frame and conversion to supervision Detections\u001b[39;00m\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;66;03m# print(f\"frame_nr: {frame_nr}\")\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m         detections \u001b[38;5;241m=\u001b[39m Detections(\n\u001b[1;32m     61\u001b[0m             xyxy\u001b[38;5;241m=\u001b[39mresults[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mxyxy\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m     62\u001b[0m             confidence\u001b[38;5;241m=\u001b[39mresults[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m     63\u001b[0m             class_id\u001b[38;5;241m=\u001b[39mresults[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mcls\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     64\u001b[0m         )\n\u001b[1;32m     65\u001b[0m         detections_people \u001b[38;5;241m=\u001b[39m Detections(\n\u001b[1;32m     66\u001b[0m             xyxy\u001b[38;5;241m=\u001b[39mresults[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mxyxy\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m     67\u001b[0m             confidence\u001b[38;5;241m=\u001b[39mresults[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m     68\u001b[0m             class_id\u001b[38;5;241m=\u001b[39mresults[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mcls\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     69\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/model.py:169\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    148\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    149\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    151\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    152\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m    An alias for the predict method, enabling the model instance to be callable.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m        (List[ultralytics.engine.results.Results]): A list of prediction results, encapsulated in the Results class.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/model.py:429\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/predictor.py:204\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/predictor.py:283\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 283\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/predictor.py:140\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    135\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    136\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    139\u001b[0m )\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/autobackend.py:384\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[1;32m    381\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:  \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    386\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:83\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:101\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:122\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 122\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    123\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/modules/block.py:226\u001b[0m, in \u001b[0;36mC2f.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    224\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    225\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(m(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[0;32m--> 226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/modules/conv.py:54\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1507\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1504\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[1;32m   1505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from cProfile import label\n",
    "from tabnanny import verbose\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "from numpy import argmax\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "def annotate_with_counts(frame, time, counts, font_scale=1, thickness=2):\n",
    "    for i, transitions in enumerate(counts.keys()):\n",
    "        if counts[transitions] > 0:\n",
    "            cv2.putText(frame, f\"{transitions[0]+1} -> {transitions[1]+1}: {counts[transitions]}\", (10, i * 30), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 0, 255), thickness)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# create BYTETracker instance\n",
    "byte_tracker = BYTETracker(BYTETrackerArgs())\n",
    "# create VideoInfo instance\n",
    "video_info = VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
    "# create frame generator\n",
    "generator = get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "# video_info = VideoInfo.from_video_path(AUX_VIDEO_PATH)\n",
    "# # create frame generator\n",
    "# generator = get_video_frames_generator(AUX_VIDEO_PATH)\n",
    "# create instance of BoxAnnotator and LineCounterAnnotator\n",
    "box_annotator = BoxAnnotator(color=ColorPalette(), thickness=1, text_thickness=1, text_scale=0.5)\n",
    "\n",
    "# # Timestamps for each frame\n",
    "# cap = cv2.VideoCapture(SOURCE_VIDEO_PATH)\n",
    "# # cap = cv2.VideoCapture(AUX_VIDEO_PATH)\n",
    "# fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "# print(f\"fps: {fps}\")\n",
    "# timestamps = [cap.get(cv2.CAP_PROP_POS_MSEC)]\n",
    "# calc_timestamps = [0.0]\n",
    "# while(cap.isOpened()):\n",
    "#     frame_exists, curr_frame = cap.read()\n",
    "#     if frame_exists:\n",
    "#         timestamps.append(cap.get(cv2.CAP_PROP_POS_MSEC))\n",
    "#         calc_timestamps.append(calc_timestamps[-1] + 1000/fps)\n",
    "#     else:\n",
    "#         break\n",
    "# cap.release()\n",
    "# frame_time_stamp = dict()\n",
    "# for i, (ts, cts) in enumerate(zip(timestamps, calc_timestamps)):\n",
    "#     if i not in frame_time_stamp:\n",
    "#         frame_time_stamp[i] = (timedelta(milliseconds=ts), ts)\n",
    "\n",
    "# statistics = pd.DataFrame(columns=[\"Frame Nr\", \"Timestamp\", \"People crossed on green\", \"Vehicles crossed on green\", \"People crossed on red\", \"Vehicles crossed on red\", \n",
    "#                                    \"Traffic light color\"])\n",
    "\n",
    "# open target video file\n",
    "detection_position = dict()\n",
    "transition_counts = dict()\n",
    "with VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
    "    # loop over video frames\n",
    "    for frame_nr, frame in enumerate(tqdm(generator, total=video_info.total_frames), start=1):        # model prediction on single frame and conversion to supervision Detections\n",
    "        # print(f\"frame_nr: {frame_nr}\")\n",
    "        results = model(frame, verbose=False)\n",
    "        detections = Detections(\n",
    "            xyxy=results[0].boxes.xyxy.cpu().numpy(),\n",
    "            confidence=results[0].boxes.conf.cpu().numpy(),\n",
    "            class_id=results[0].boxes.cls.cpu().numpy().astype(int)\n",
    "        )\n",
    "        detections_people = Detections(\n",
    "            xyxy=results[0].boxes.xyxy.cpu().numpy(),\n",
    "            confidence=results[0].boxes.conf.cpu().numpy(),\n",
    "            class_id=results[0].boxes.cls.cpu().numpy().astype(int)\n",
    "        )\n",
    "        detections_vehicles = Detections(\n",
    "            xyxy=results[0].boxes.xyxy.cpu().numpy(),\n",
    "            confidence=results[0].boxes.conf.cpu().numpy(),\n",
    "            class_id=results[0].boxes.cls.cpu().numpy().astype(int)\n",
    "        )\n",
    "        detections_traffic_light = Detections(\n",
    "            xyxy=results[0].boxes.xyxy.cpu().numpy(),\n",
    "            confidence=results[0].boxes.conf.cpu().numpy(),\n",
    "            class_id=results[0].boxes.cls.cpu().numpy().astype(int)\n",
    "        )\n",
    "        # filtering out detections with unwanted classes\n",
    "        mask_people = np.array([class_id in CLASS_ID_PEOPLE for class_id in detections.class_id], dtype=bool)\n",
    "        mask_vehicle = np.array([class_id in CLASS_ID_VEHICLE for class_id in detections.class_id], dtype=bool)\n",
    "        mask_traffic_light= np.array([class_id in CLASS_ID_TRAFFIC_LIGHT for class_id in detections.class_id], dtype=bool)\n",
    "        detections_people.filter(mask=mask_people, inplace=True)\n",
    "        detections_vehicles.filter(mask=mask_vehicle, inplace=True)\n",
    "        detections_traffic_light.filter(mask=mask_traffic_light, inplace=True)\n",
    "        mask = np.array([class_id in CLASS_ID for class_id in detections.class_id], dtype=bool)\n",
    "        detections.filter(mask=mask, inplace=True)\n",
    "        # tracking detections\n",
    "        tracks_people = byte_tracker.update(\n",
    "            output_results=detections2boxes(detections=detections_people),\n",
    "            img_info=frame.shape,\n",
    "            img_size=frame.shape\n",
    "        )\n",
    "        tracks_vehicles = byte_tracker.update(\n",
    "            output_results=detections2boxes(detections=detections_vehicles),\n",
    "            img_info=frame.shape,\n",
    "            img_size=frame.shape\n",
    "        )\n",
    "        tracks = byte_tracker.update(\n",
    "            output_results=detections2boxes(detections=detections),\n",
    "            img_info=frame.shape,\n",
    "            img_size=frame.shape\n",
    "        )\n",
    "        tracker_id = match_detections_with_tracks(detections=detections, tracks=tracks)\n",
    "        tracker_id_people = match_detections_with_tracks(detections=detections_people, tracks=tracks_people)\n",
    "        tracker_id_vehicles = match_detections_with_tracks(detections=detections_vehicles, tracks=tracks_vehicles)\n",
    "        detections.tracker_id = np.array(tracker_id)\n",
    "        detections_people.tracker_id = np.array(tracker_id_people)\n",
    "        detections_vehicles.tracker_id = np.array(tracker_id_vehicles)\n",
    "        # filtering out detections without trackers\n",
    "        mask = np.array([tracker_id is not None for tracker_id in detections.tracker_id], dtype=bool)\n",
    "        mask_people = np.array([tracker_id is not None for tracker_id in detections_people.tracker_id], dtype=bool)\n",
    "        mask_vehicles = np.array([tracker_id is not None for tracker_id in detections_vehicles.tracker_id], dtype=bool)\n",
    "        detections.filter(mask=mask, inplace=True)\n",
    "        detections_people.filter(mask=mask_people, inplace=True)\n",
    "        detections_vehicles.filter(mask=mask_vehicles, inplace=True)\n",
    "\n",
    "        # Get traffic zones count\n",
    "        traffic_zones, detection_in_zone, counts = count_trajectories(polygons, detections, detection_position)\n",
    "        for _, confidence, class_id, tracker_id in detections:\n",
    "            if tracker_id is not None and detection_in_zone.get(tracker_id, -1) >= 0:\n",
    "                detection_position[tracker_id] = detection_in_zone.get(tracker_id, -1)\n",
    "        for count in counts.keys():\n",
    "            if count not in transition_counts.keys():\n",
    "                transition_counts[count] = 0\n",
    "            transition_counts[count] += counts[count]\n",
    "        labels = [\n",
    "            f\"#{tracker_id} {detection_in_zone.get(tracker_id, -1) + 1}\" if tracker_id is not None else f\"#{tracker_id}\"\n",
    "            for _, confidence, class_id, tracker_id\n",
    "            in detections\n",
    "        ]\n",
    "        box_annotator.annotate(frame=frame, detections=detections, labels=labels)\n",
    "        # Write in the center of the polygon the number of the polygon\n",
    "        for i, polygon in enumerate(polygons):\n",
    "            x = 0\n",
    "            y = 0\n",
    "            for point in polygon:\n",
    "                x += point.x\n",
    "                y += point.y\n",
    "            x = int(x / len(polygon))\n",
    "            y = int(y / len(polygon))\n",
    "            cv2.putText(frame, f\"{i+1}\", (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        # Annotate with counts\n",
    "        annotate_with_counts(frame, 0, transition_counts)\n",
    "        # Draw the polygons\n",
    "        for i, polygon in enumerate(polygons):\n",
    "            color = (255, 0, 0)\n",
    "            if i == 0:\n",
    "                color = (0, 255, 0)\n",
    "            for j in range(len(polygon)):\n",
    "                cv2.line(frame, (polygon[j].x, polygon[j].y), (polygon[(j+1) % len(polygon)].x, polygon[(j+1) % len(polygon)].y), color, 2)\n",
    "        sink.write_frame(frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
